# C6EnPLS dataset
C6EnPLS is a dataset of HPC jobs executed on CRESCO6 infrastructure. Each job refers to the execution of a linear solver. 

### If you find this dataset useful please consider citing:
Artioli, M.; Borghesi, A.; Chinnici, M.; Ciampolini, A.; Colonna, M.; De Chiara, D.; Loreti, D. C6EnPLS: A High-Performance Computing Job Dataset for the Analysis of Linear Solvers’ Power Consumption. Future Internet 2025, 17, 203. https://doi.org/10.3390/fi17050203

### The repository is structured as follows.
- **RAW_C6EnPLS folder**: reports all the data collected during the job runs. Some information might be replicated or difficult to locate there.
- **Test_scripts**: all the system scripts that allowed the collection of the data, including pre-execution and post-execution phases as described in [\[1\]](#paper)
- **CLEAN_C6EnPLS** folder: orderly version of RAW_C6EnPLS. Here you can find:
    - **jobs_spec** folder: specifics of each launched job, i.e., algorithm, number of executing/spare/total processes employed, distribution over the nodes' sockets, matrix size, fault tolerance level, number of injected faults and their location, etc.
    - **jobs_data** folder: all the data collected about each job run (i.e., jobid, starting/ending timestamp, assigned physical nodes). Data from different days are stored in different files.
    - **sensors_data** folder: all the measurements collected by the sensors across the whole infrastructure. Measures from different days are stored in different files.
    - **results** folder: outputs, errors and information regarding each job run.
    - **sensors_measures_extraction.ipynb**: a jupyter notebook to extract valuable information from the sensors_data folder and produce the **sensors_measures.csv** file. This CSV file contains the sensors' measurements, each reporting the monitored node and the job-id of the job that was running on that node at the time of the measurement. For a detailed explanation of the fields, please refer to [\[2\]](#future)
    - **jobs_data_extraction.ipynb**: a jupyter notebook to extract valuable information from the jobs_data folder and produce the **jobs_data.csv** file. This CSV file reports all the information regarding the launch of any job. Fields have the following meanings:
        - jobid, unique id of the job 
        - jobname, given name of the job
        - start_ts and end_ts, recorded start and end timestamps of the job
        - nodes, name of the nodes involved in the computation and number of processors used on each node in the format \<numer_of_processors\>*\<name_of_node1\>:\<numer_of_processors\>*\<name_of_node2\>: etc.
        - condition_n, condition number of the computed matrix
        - error_code, error code generated by the computation \(0 for no error\)
        - runtime, whole execution run time (as reported by LSF scheduler)
        - init_and_call_runtime, runtime measured with specific calls in the code (whole execution of the routine)
        - call_runtime, runtime measured with specific calls in the code (execution of the routine without the initialization part, i.e., without initial dispatching of the matrix blocks to the nodes going to elaborate them)
        - algorithm, the executed solver
        - fault_tolerance, the level of algorithm-based fault tolerance set for the job, i.e., number of processors that could encounter a hard fault without invalidating the whole computation
        - fault_happened, the actual number of faults injected in the computation and recovered with the algorithm-specific procedure
        - computing_proc, the number of processors involved in the computation
        - total_proc, the total number of processors involved in the computation. It can be higher than computing_proc if some nodes are used as spare nodes to ensure fault tolerance
        - matrix_size, size of the matrix 
        - precision, double or single floating-point precision
        - balanced, \[y,n\] if the computation is conducted with the same number of computing processors on each of the two sockets of all involved nodes
        - repetition, repetition count 
        - req_whole_time, estimated whole-time of the job provided by the user at submission time
        - number_of_nodes, the total number of physical nodes involved in the computation
        - node1,...,node16, number of cores used on each physical node
    - **jobs_info_extraction.ipynb**: a jupyter notebook to join the data from sensors and jobs. It reads both sensors_measures.csv and jobs_data.csv, and composes **jobs_info.csv**. This CSV file contains the computed summary of sensors_measures.csv and jobs_data.csv, reporting the values of energy, runtime, mean and maximum system power of each job run.
    - **graphs.ipynb** generates visual representations of the data distributions.
    

### References
<a name="paper"></a> \[1\] Artioli, M.; Borghesi, A.; Chinnici, M.; Ciampolini, A.; Colonna, M.; De Chiara, D.; Loreti, D. C6EnPLS: A High-Performance Computing Job Dataset for the Analysis of Linear Solvers’ Power Consumption. Future Internet 2025, 17, 203. https://doi.org/10.3390/fi17050203

<a name="future"></a> \[2\] Yibrah Gebreyesus, Damian Dalton, Sebastian Nixon, Davide De Chiara and Marta Chinnici, Machine Learning for Data Center Optimizations: Feature Selection Using Shapley Additive exPlanation (SHAP). Future Internet 2023, 15, 88. https://doi.org/10.3390/fi15030088
